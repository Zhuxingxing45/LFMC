# üß† LFMC: Enhancing LLMs' Logical Reasoning through Mistake Correction

## üìñ È°πÁõÆÁÆÄ‰ªã

Âú®ËøëÂπ¥Êù•Ôºå**Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ** Áî±‰∫éÂÖ∂Âº∫Â§ßÁöÑÊñáÊú¨ÁîüÊàêËÉΩÂäõËÄåË¢´ÂπøÊ≥õÂ∫îÁî®„ÄÇ‰ΩÜÂÆÉ‰ª¨Âú®ÈÄªËæëÊé®ÁêÜ‰∏≠‰ªçÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØ**Áº∫‰πèÂÉè‰∫∫Á±ª‰∏ÄÊ†∑ÈÄöËøáÈîôËØØÂèçÊÄùÊù•ÊèêÂçáÊé®ÁêÜËÉΩÂäõ**„ÄÇ

Êú¨È°πÁõÆÊèêÂá∫ **LFMC (Logic Fine-tuning with Mistake Correction)** ÊñπÊ≥ïÔºö

* ‰ΩøÁî® GPT-4 Ëá™Âä®‰øÆÊ≠£ÂåÖÂê´ÈÄªËæëÈîôËØØÁöÑÊé®ÁêÜË∑ØÂæÑ
* ÊûÑÂª∫ **LOCD (Logical Error Correction Dataset)** Êï∞ÊçÆÈõÜ
* ‰ΩøÁî®LOCDÔºåÈÄöËøá **QLoRA** È´òÊïàÂæÆË∞ÉÊèêÂçáÂ§öÁßçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ

ÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî® LOCD ÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÂú®Âõõ‰∏™ÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏äÂùáË∂ÖË∂ä‰∫ÜÂü∫Á∫øÊ®°ÂûãÔºåËØÅÊòé‰∫Ü LLMs ËÉΩÂ§üÈÄöËøáÈîôËØØ‰øÆÊ≠£Â≠¶‰π†Êõ¥Á®≥ÂÅ•ÁöÑÈÄªËæëÊé®ÁêÜ„ÄÇ

---

## üìÇ Project Structure

```
LFMC/
‚îú‚îÄ‚îÄ data/                # Datasets (original logical questions & corrected reasoning paths)
‚îú‚îÄ‚îÄ root/                # Model weights and configurations
‚îú‚îÄ‚îÄ config/              # QLoRA fine-tuning scripts
‚îú‚îÄ‚îÄ results/             # Experimental results
‚îú‚îÄ‚îÄ requirements.txt     # Project dependencies
‚îî‚îÄ‚îÄ README.md            # Project documentation
```


## üöÄ LOCDÊï∞ÊçÆÈõÜÊûÑÂª∫

* üîç **ÈÄªËæëÈîôËØØ‰øÆÊ≠£**ÔºöÂà©Áî® GPT-4 ÁîüÊàêÊ≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑ
* üìä **LOCD Êï∞ÊçÆÈõÜÊûÑÂª∫**ÔºöÂéüÂßãÈÄªËæëÈóÆÈ¢ò + GPT-4 ‰øÆÊ≠£ËæìÂá∫
* ‚ö° **È´òÊïàÂæÆË∞É**ÔºöÈÄöËøá QLoRA ÂØπ LLaMA3-8B ËøõË°åÂèÇÊï∞È´òÊïàÂæÆË∞É
* üß™ **ÂÆûÈ™åÈ™åËØÅ**ÔºöÂú®Âõõ‰∏™ÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏äÊòæËëóÊèêÂçáÊÄßËÉΩ

---

## üì¶ Installation & Usage

### Environment

* GPU: NVIDIA GeForce RTX 4090
* Python >= 3.10
* CUDA >= 12.1 
* Dependencies: Listed in `requirements.txt`

### Installation

```bash
# Clone the repository:
git clone git@github.com:Zhuxingxing45/LFMC.git
cd LFMC

# ÂàõÂª∫ËôöÊãüÁéØÂ¢É (Êé®Ëçê)
conda create -n yourenv python=3.10
conda activate yourenv
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia
pip install -r requirements.txt
```

### Usage
## Training
This section describes how to fine-tune a single model  with the training configuration.

### 1. Training a Single Model
All training configurations are located in the `configs` directory.  
For example, to train the LLaMA3-8B model with LOCD data:
```bash
# Train the model
xtuner train configs/llama/llama3_correction/llama3_8b_instruct_qlora_logic_correct_ez.py \
    --work-dir root/llama3-8b/llama3_logic_lfud/llama3_logic_original_pth
```

Convert checkpoint to HuggingFace adapter:
```bash
xtuner convert pth_to_hf <config_path> <checkpoint_path> <adapter_output_path>
```


Merge with base model (optional):

```bash
export MKL_SERVICE_FORCE_INTEL=1
xtuner convert merge <base_model_path> <adapter_path> <merged_model_path>
```

---

## **Evaluation**

Evaluate the fine-tuned model:

```bash
python logic_llm/qwen3/evaluate.py \
    --model_path <merged_model_path> \
    --output_path <generate_data_path> \
    --result_path <accuracy_json_path>
```

---

## **Data**

* LOCD dataset located in `./data/LOCD` (training/validation/test splits included)
* External datasets used: 
    * FOLIO: [https://github.com/Yale-LILY/FOLIO](https://github.com/Yale-LILY/FOLIO)
  * ReClor: [https://whyu.me/reclor/](https://whyu.me/reclor/)
  * LogiQA_v2: [https://github.com/csitfun/LogiQA2.0](https://github.com/csitfun/LogiQA2.0)
  * logiqa-zh: [https://doi.org/10.48550/arXiv.2007.08124](https://doi.org/10.48550/arXiv.2007.08124)
  * LogiCoT: [https://github.com/csitfun/LogiCoT](https://github.com/csitfun/LogiCoT)
  * LFUD: [https://github.com/YandaGo/LFUD](https://github.com/YandaGo/LFUD)
---








