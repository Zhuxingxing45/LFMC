# ðŸ§  LFMC: Enhancing LLMs' Logical Reasoning through Mistake Correction

## Title

**LFMC (Logic Fine-tuning with Mistake Correction): Improving Logical Reasoning of Large Language Models via Mistake Correction**

## Description

Large Language Models (LLMs) are widely used for text generation but still exhibit deficiencies in **logical reasoning**, especially their **inability to self-improve through reflection on errors**.

This project introduces **LFMC**, a fine-tuning framework designed to enhance LLMsâ€™ reasoning capabilities by learning from **mistake correction**:

- Automatically detects and corrects reasoning errors using GPT-4.
- Constructs the **LOCD (Logical Correction Dataset)**.
- Fine-tunes models with **QLoRA** to efficiently improve reasoning consistency and logical accuracy.

Experiments across multiple benchmarks (FOLIO, ReClor, LogiQA, LFUD, etc.) show that LFMC-fine-tuned models outperform standard instruction-tuned baselines, confirming that **reasoning through correction** leads to better logical generalization.

---

## Project structure

```
LFMC/
â”œâ”€â”€ data/                # Datasets (original logical questions & corrected reasoning)
â”œâ”€â”€ root/                # Model weights and configurations
â”œâ”€â”€ config/              # QLoRA fine-tuning scripts
â”œâ”€â”€ logic_llm/           
â”‚   â”œâ”€â”€ wrong_reasoning_collection/    # Collect erroneous reasoning
â”‚   â”œâ”€â”€ evaluate/                      # Evaluation scripts
â”‚   â”œâ”€â”€ results/                       # Save test results
â”œâ”€â”€ gpt4_correct/        # GPT-4 correction pipeline
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md
```

---

## Dataset Information

**Dataset name:** LOCD (Logical Correction Dataset)

- **Source:** Collected from reasoning errors generated by various LLMs, then corrected by GPT-4.
- **Format:** JSONL (each sample includes question, reasoning, error type, and corrected reasoning).
- **Splits:** `train.jsonl`, `dev.jsonl`, `test.jsonl`.
- **External datasets used for evaluation:** FOLIO, ReClor, LogiQA_v2, logiqa-zh, LogiCoT, LFUD.

---

## Code Information

**Algorithms implemented:**

1. Error Collection â€“ generate reasoning chains and collect those with logical mistakes.
2. Error Correction â€“ use GPT-4 to correct flawed reasoning paths (`gpt4_correct/logic_correction_v2.py`).
3. Fine-tuning â€“ QLoRA-based fine-tuning (configs in `config/`).
4. Evaluation â€“ benchmark evaluation (`logic_llm/evaluate/`).

---

## Requirements

- **GPU:** NVIDIA GeForce RTX 4090 (recommended)
- **Python:** >= 3.10
- **CUDA:** >= 12.1
- **Core dependencies:** see `requirements.txt`

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## Usage Instructions

### 1. Clone the repository

```bash
git clone git@github.com:Zhuxingxing45/LFMC.git
cd LFMC
```

### 2. Create a virtual environment (recommended)

```bash
conda create -n lfmc python=3.10
conda activate lfmc
```

### 3. Generate corrected reasoning data

```bash
python gpt4_correct/logic_correction_v2.py     --data_path ./data/Wrong_Inference     --dataset_name logiqa_zh     --api_key <YOUR_OPENAI_API_KEY>
```

### 4. Fine-tune the model

Example:

```bash
xtuner train configs/llama/llama3_correction/llama3_8b_instruct_qlora_logic_correct_ez.py     --work-dir root/llama3-8b/lfmc_logic_correct/
```

Convert checkpoint:

```bash
xtuner convert pth_to_hf <config_path> <checkpoint_path> <adapter_output_path>
```

### 5. Evaluate

```bash
python logic_llm/evaluate/evaluate.py     --model_path <merged_model_path>     --output_path ./results/generated_data     --result_path ./results/accuracy.json
```

---

## Methodology

1. Collect logical errors by sampling model outputs and filtering inconsistent reasoning.
2. Use GPT-4 to correct reasoning chains while keeping semantic intent.
3. Construct LOCD combining original & corrected chains for supervised fine-tuning.
4. Fine-tune with QLoRA and low-rank adapters.
5. Evaluate on multiple reasoning benchmarks using accuracy and chain-of-thought metrics.

---

## Reproduction Script

A helper shell script `scripts/reproduce_all.sh` is included to automate:

- Dataset preparation
- Data correction (GPT-4)
- Model fine-tuning
- Evaluation
- Result aggregation

Run:

```bash
bash scripts/reproduce_all.sh
```

> **Note:** The script assumes configuration files and data paths exist and that you have valid API keys and sufficient GPU resources.

---

## Citations

If you use LFMC or LOCD, please cite:

```
@article{zhu2025lfmc,
  title={LFMC: Enhancing Large Language Models' Logical Reasoning through Mistake Correction},
  author={Zhu, Xingxing and Huang, Xiaoxi},
  year={2025},
  journal={Under Review}
}
```

---

## License

This project is licensed under the MIT License. See `LICENSE` for details.

---

## Contribution Guidelines

1. Fork the repo
2. Create a branch `feature/...`
3. Open a PR with description and tests
4. Keep code style consistent and document changes

---
