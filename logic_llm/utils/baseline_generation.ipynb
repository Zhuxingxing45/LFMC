{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"490eb9ba27a84a52a255355f2bcbe4d0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import transformers\n","import torch\n","\n","model_id = \"/home/23_zxx/workspace/llama3-ft/Meta-Llama-3-8B-Instruct\"\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model = model_id,\n","    model_kwargs = {\"torch_dtype\":torch.bfloat16},\n","    device = \"cuda\",\n",")\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'id': 'ProntoQA_1', 'context': 'Jompuses are not shy. Jompuses are yumpuses. Each yumpus is aggressive. Each yumpus is a dumpus. Dumpuses are not wooden. Dumpuses are wumpuses. Wumpuses are red. Every wumpus is an impus. Each impus is opaque. Impuses are tumpuses. Numpuses are sour. Tumpuses are not sour. Tumpuses are vumpuses. Vumpuses are earthy. Every vumpus is a zumpus. Zumpuses are small. Zumpuses are rompuses. Max is a yumpus.', 'question': 'Is the following statement true or false? Max is sour.', 'options': ['A) True', 'B) False'], 'answer': 'B', 'explanation': ['Max is a yumpus.', 'Each yumpus is a dumpus.', 'Max is a dumpus.', 'Dumpuses are wumpuses.', 'Max is a wumpus.', 'Every wumpus is an impus.', 'Max is an impus.', 'Impuses are tumpuses.', 'Max is a tumpus.', 'Tumpuses are not sour.', 'Max is not sour.']}\n"]}],"source":["import os\n","import json\n","def load_raw_dataset(data_path, data_name, split):\n","    with open(os.path.join(data_path, data_name, f'{split}.json')) as f:\n","        raw_dataset = json.load(f)\n","    return raw_dataset\n","\n","raw_dataset = load_raw_dataset('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/data','ProntoQA','dev') \n","print(raw_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_in_context_examples(demonstration_path,dataset_name,mode):\n","    with open(os.path.join(demonstration_path, f'{dataset_name}_{mode}.txt')) as f:\n","        in_context_examples = f.read()\n","    return in_context_examples\n","\n","in_context_example = load_in_context_examples('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/logic_llm/icl_examples','ProntoQA', 'Direct' )\n","print(in_context_example)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def prompt_creator(in_context_example, example):\n","    full_prompt = in_context_example\n","    context = example['context']\n","    question = example['question']\n","    options = '\\n'.join([opt.strip() for opt in example['options']])\n","    full_prompt = full_prompt.replace('[[CONTEXT]]', context)\n","    full_prompt = full_prompt.replace('[[QUESTION]]', question)\n","    full_prompt = full_prompt.replace('[[OPTIONS]]', options)\n","    return full_prompt"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 500 examples from split.\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 10%|█         | 1/10 [00:03<00:27,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 20%|██        | 2/10 [00:04<00:16,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 30%|███       | 3/10 [00:08<00:21,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 40%|████      | 4/10 [00:12<00:19,  3.21s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 50%|█████     | 5/10 [00:19<00:24,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 60%|██████    | 6/10 [00:26<00:21,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 70%|███████   | 7/10 [00:28<00:13,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 80%|████████  | 8/10 [00:35<00:10,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"," 90%|█████████ | 9/10 [00:38<00:04,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","100%|██████████| 10/10 [00:41<00:00,  4.14s/it]\n"]}],"source":["from tqdm import tqdm\n","def reasoning_graph_generation(save_path, mode, dataset_name, split, model_name):\n","    raw_dataset = load_raw_dataset('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/data','ProntoQA','dev')\n","    print(f\"Loaded {len(raw_dataset)} examples from split.\")\n","\n","    in_context_example = load_in_context_examples('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/logic_llm/icl_examples','ProntoQA', 'Direct' )\n","\n","    outputs = []\n","    generated_texts = []\n","    for example in tqdm(raw_dataset[0:10]):\n","        question = example['question']\n","\n","        # create prompt\n","        full_prompt = prompt_creator(in_context_example, example)\n","        messages = [\n","            {'role':'system','content': 'hello,You are a helpful human assistant!'},\n","            {'role':'user', 'content': full_prompt }\n","        ]\n","        prompt = pipeline.tokenizer.apply_chat_template(\n","            messages,\n","            tokenize=False,\n","            add_generation_prompt = True\n","        )\n","\n","        terminators = [\n","            pipeline.tokenizer.eos_token_id,\n","            pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","        ]\n","\n","        generated_text = pipeline(\n","            prompt,\n","            max_new_tokens = 256,\n","            eos_token_id = terminators,\n","            do_sample = True,\n","            temperature = 0.6,\n","            top_p = 0.9\n","        )\n","        generated_texts.append(generated_text)\n","\n","        response = generated_text[0]['generated_text'].split('<|start_header_id|>assistant<|end_header_id|>')[-1]\n","        label_phrase = 'correct option is:'\n","        generated_reasoning = response.split(label_phrase)[0].strip()\n","        generated_answer = response.split(label_phrase)[-1].strip()\n","        output = {'id': example['id'], \n","                'question': question, \n","                'answer': example['answer'], \n","                'predicted_reasoning': generated_reasoning,\n","                'predicted_answer': generated_answer}\n","        outputs.append(output)\n","\n","    with open(os.path.join(save_path, f'{dataset_name}_generation.json'), 'w') as f:\n","        json.dump(generated_texts, f, indent=2, ensure_ascii=False)\n","\n","    # save outputs        \n","    with open(os.path.join(save_path, f'{mode}_{dataset_name}_{split}_{model_name}.json'), 'w') as f:\n","        json.dump(outputs, f, indent=2, ensure_ascii=False)\n","    \n","reasoning_graph_generation('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/logic_llm/results', 'Direct', 'ProntoQA', 'dev', 'Llama3-8B-Instruction')        \n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[": B) False.\n","prediction: None \t gold_answers: B \t match: False\n","prediction: B \t gold_answers: A \t match: False\n","prediction: B \t gold_answers: A \t match: False\n","prediction: B \t gold_answers: B \t match: True\n","I'd be happy to help!\n","\n","From the context, we know that Alex is a numpus, and Wumpuses are impuses. Since Alex is a numpus, and each numpus is a tumpus, Alex is also a tumpus. We also know that Wumpuses are not dull, and Impuses are Wumpuses. Therefore, Impuses are not dull. Since Impuses are also dumpuses, and dumpuses are not wooden, Impuses are not wooden. However, we don't have any information about the dullness of dumpuses.\n","\n","Now, let's look at the question: Is the following statement true or false? Alex is not dull.\n","\n","From the context, we know that Wumpuses are not dull, and Impuses are Wumpuses. Since Alex is a tumpus, and each tumpus is not large, Alex is not large. However, we don't have any information about the dullness of tumpuses. Since Alex is a numpus, and each numpus is a tumpus, Alex is also a tumpus. However, we don't have any information about the dullness of tumpuses.\n","\n","Since we don't have enough information to determine\n","prediction: None \t gold_answers: A \t match: False\n","prediction: B \t gold_answers: A \t match: False\n","prediction: B \t gold_answers: B \t match: True\n","prediction: B \t gold_answers: B \t match: True\n","prediction: B \t gold_answers: A \t match: False\n","prediction: B \t gold_answers: B \t match: True\n","EM: 0.4\n"]}],"source":["import re\n","import json\n","from tqdm import tqdm\n","import random\n","import os\n","import argparse\n","\n","def extract_number(string):\n","    # Remove all characters except digits, decimal point and negative sign\n","    try:\n","        num_string = re.sub(r'[^\\d.-]', '', string)\n","        num_string = num_string.replace('$', '')\n","        return float(num_string)\n","    except:\n","        try:\n","            return float(random.randint(0, 100))\n","            # return float(w2n.word_to_num(string))\n","        except:\n","            # print('Error: ', string)\n","            print('Error')\n","            return float(random.randint(0, 100))\n","\n","def argmax(iterable):\n","    return max(enumerate(iterable), key=lambda x: x[1])[0]\n","\n","# these functions are heavily influenced by the HF squad_metrics.py script\n","def normalize_text(s):\n","    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n","    import string, re\n","\n","    def remove_articles(text):\n","        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","        return re.sub(regex, \" \", text)\n","\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","def compute_exact_match(prediction, truth):\n","    return int(normalize_text(prediction) == normalize_text(truth))\n","    # return prediction == truth\n","\n","def compute_f1(prediction, truth):\n","    pred_tokens = normalize_text(prediction).split()\n","    truth_tokens = normalize_text(truth).split()\n","    \n","    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n","    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n","        return int(pred_tokens == truth_tokens)\n","    \n","    common_tokens = set(pred_tokens) & set(truth_tokens)\n","    \n","    # if there are no common tokens then f1 = 0\n","    if len(common_tokens) == 0:\n","        return 0\n","    \n","    prec = len(common_tokens) / len(pred_tokens)\n","    rec = len(common_tokens) / len(truth_tokens)\n","    \n","    return 2 * (prec * rec) / (prec + rec)\n","\n","def evaluate_sample(prediction, gold_answers):\n","    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n","    f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n","    return em_score, f1_score\n","\n","def get_choice(answer_str):\n","    choices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'A)', 'B)', 'C)', 'D)', 'E)', 'F)', 'G)', 'H)', \n","               'A.', 'B.', 'C.', 'D.', 'E.', 'F.', 'G.', 'H.']\n","    for c in choices:\n","        if answer_str.startswith(c):\n","            return c.replace(')', '')\n","    return None\n","\n","def evaluate_QA(result_file):\n","    with open(result_file, 'r') as f:\n","        QA_results = json.load(f)\n","\n","    total_em = 0.0\n","    total_f1 = 0.0\n","    count = 0\n","    for sample in QA_results:\n","        gold_answer = sample['answer'].replace('(', '').replace(')', '').strip()\n","        answer_str = sample['predicted_answer'].strip()\n","        prediction = get_choice(answer_str)\n","\n","        indicators = ['the correct option is', 'the correct answer is', \n","                      'The correct answer is', 'The correct option is',\n","                      'Thus, the answer is']\n","        if prediction is None:\n","            for indicator in indicators:\n","                if answer_str.find(indicator)>=0:\n","                    answer_str = answer_str.split(indicator)[1].strip()\n","                    prediction = get_choice(answer_str)\n","                    break\n","\n","        if prediction is None:\n","            print(answer_str)\n","\n","        print(f\"prediction: {prediction} \\t gold_answers: {gold_answer} \\t match: {prediction == gold_answer}\")\n","        \n","        em_score = 1.0 if prediction == gold_answer else 0.0\n","        total_em += em_score\n","        count += 1\n","    \n","    avg_em = total_em / count\n","    print(f\"EM: {avg_em}\")\n","\n","# def parse_args():\n","#     parser = argparse.ArgumentParser()\n","#     parser.add_argument('--dataset_name', type=str)\n","#     parser.add_argument('--model_name', type=str)\n","#     parser.add_argument('--mode', type=str)\n","#     parser.add_argument('--split', type=str, default='dev')\n","#     parser.add_argument('--result_path', type=str, default='./results')\n","#     args = parser.parse_args()\n","#     return args\n","\n","if __name__ == \"__main__\":\n","    #args = parse_args()\n","    result_file = os.path.join('/home/23_zxx/workspace/llama3-ft/Llama3-Tutorial/logic_llm/results', 'Direct_ProntoQA_dev_Llama3-8B-Instruction.json')\n","    evaluate_QA(result_file)\n"]}],"metadata":{"kernelspec":{"display_name":"llama3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}
